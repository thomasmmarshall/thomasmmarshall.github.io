---
layout: post
title: NewtonianVAE ELBO Derivation
date: 2022-11-10 11:12:00-0400
description: The derivation of the evidence lower bound of the newtonian variational autoencoder model from my Master's thesis.
tags: variational-inference robotics vision-based bayesian maths derivation
categories: academic
---


This is the derivation as presented by [Jaques et al. (2021)](https://arxiv.org/abs/2006.01959), with minor corrections. Once again we consider the marginal data likelihood:

$$p(\mathbf{x}_{1:T} \mid \mathbf{u}_{1:T}) = \int p(\mathbf{x}_{1:T} \mid \mathbf{z}_{1:T} \mid \mathbf{u}_{1:T}) p(\mathbf{z}_{1:T} \mid \mathbf{u}_{1:T}) \, \text{d}\mathbf{z}_{1:T} \, .
$$

The variable $$\mathbf{\hat{z}}$$ refers to the estimate of the positional latent variable $$\mathbf{z}$$ at time $$t$$ as per the learned transition dynamics at that stage. The two factors in the integral are further factorised as follows:

$$
\begin{split}
p(\mathbf{x}_{1:T} \mid \mathbf{z}_{1:T}, \mathbf{u}_{1:T}) &= \prod_t p(\mathbf{x}_{t} \mid \mathbf{z}_{t}) \\
&= \prod_t \int p(\mathbf{x}_t \mid \mathbf{\hat{z}}_t) p(\mathbf{\hat{z}}_{t-1}, \mathbf{u}_{t-1}; \mathbf{v}_{t-1}) \, \text{d}\mathbf{z}_t
\end{split}
$$

and,

$$
p(\mathbf{z}_{1:T} \mid \mathbf{u}_{1:T}) = \prod_t p(\mathbf{z}_t \mid \mathbf{z}_{t-1}, \mathbf{u}_{t-1}; \mathbf{v}_{t-1}) \, .
$$

where velocity is calculated as $$\mathbf{v}_t = (\mathbf{z}_t - \mathbf{z}_{t-1})/\Delta t$$. The approximate posterior of the sequence is given by:

$$
q(\mathbf{z}_{1:T} \mid \mathbf{x}_{1:T}) = \prod_t q(\mathbf{z}_t \mid \mathbf{x}_t) \, .
$$

We take the log, introduce the approximate posterior and apply Jensen's inequality:
$$
\begin{split}
&\log p(\mathbf{x}_{1:T} \mid \mathbf{u}_{1:T}) = \\
&\log \bigg( \int \prod_t \frac{ q(\mathbf{z}_t \mid \mathbf{x}_t)}{ q(\mathbf{z}_t \mid \mathbf{x}_t)} \int p(\mathbf{x}_t \mid \mathbf{\hat{z}}_t) p(\mathbf{\hat{z}}_{t-1}, \mathbf{u}_{t-1}; \mathbf{v}_{t-1}) \, \text{d}\mathbf{\hat{z}}_t \, \\
&\hspace{1.5cm} \prod_t p(\mathbf{z}_t \mid \mathbf{z}_{t-1}, \mathbf{u}_{t-1}; \mathbf{v}_{t-1}) \, \text{d}\mathbf{z}_{1:T} \bigg) \\ 
&\geq \int \prod_t q(\mathbf{z}_t \mid \mathbf{x}_t) \bigg( \sum_t \log \bigg[ \int p(\mathbf{x}_t \mid \mathbf{\hat{z}}_t) p(\mathbf{\hat{z}}_{t-1}, \mathbf{u}_{t-1}; \mathbf{v}_{t-1}) \, \text{d}\mathbf{\hat{z}}_t \\
&\hspace{1.5cm} + \sum_t \log \frac{p(\mathbf{z}_t \mid \mathbf{z}_{t-1}, \mathbf{u}_{t-1})}{(\mathbf{z}_t \mid \mathbf{x}_t)} \bigg] \bigg) \, \text{d}\mathbf{z}_{1:T} \\
&= \sum_t \int q(\mathbf{z}_{t-1} \mid \mathbf{x}_{t-1}) q(\mathbf{z}_{t-2} \mid \mathbf{x}_{t-2}) \bigg( \log \bigg[ \int p(\mathbf{x}_t \mid \mathbf{\hat{z}}_t) p(\mathbf{\hat{z}}_{t-1}, \mathbf{u}_{t-1}; \mathbf{v}_{t-1}) \, \text{d}\mathbf{\hat{z}}_t \bigg] \\
&\hspace{1.5cm} -\text{KL}(q(\mathbf{z}_t \mid \mathbf{x}_t)||p(\mathbf{z}_t \mid \mathbf{z}_{t-1}, \mathbf{u}_{t-1}) \bigg) \, \text{d}\mathbf{z}_{t} \\
&\geq \sum_t \mathbb{E}_{q(\mathbf{z}_{t-1} \mid \mathbf{x}_{t-1}) q(\mathbf{z}_{t-2} \mid \mathbf{x}_{t-2})} \bigg( \underbrace{\mathbb{E}_{p(\mathbf{\hat{z}}_{t} \mid \mathbf{z}_{t-1}, \mathbf{u}_{t-1})} \log(\mathbf{x}_t \mid \mathbf{\hat{z}}_t)}_{\text{predicted present reconstruction}} \\
&\hspace{2cm} -\underbrace{\text{KL}(q(\mathbf{z}_t \mid \mathbf{x}_t)||p(\mathbf{z}_t \mid \mathbf{z}_{t-1}, \mathbf{u}_{t-1})}_{\text{transition prior divergence}} \bigg)  \, . \\
\end{split}
$$