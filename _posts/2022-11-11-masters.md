---
layout: post
title: IN PROGESS The limitations of variational models for vision-based robotics
date: 2022-11-11 11:12:00-0400
description: This blog post serves as a severely compressed version of what I did in my Master's thesis.
tags: variational-inference robotics vision-based bayesian
categories: academic
published: false
---



# Introduction
We consider the vision-based control of robots. In vision-based robotics, a robot's state is inferred from visual data and utilised in conjunction with a control strategy to carry out intended activities. 
In [Jaques et al. (2021)](https://arxiv.org/abs/2006.01959), 

In the rest of this blog post, we'll briefly go over the three aforementioned things that we need to control robots from vision. We'll start with a very quick recap of basic control theory.

<!-- [<img src="/assets/img/feedback.jpg" width="750"/>](/assets/img/feedback.jpg) -->

# Newtonian variational autoencoder

When the data under consideration are sequences, the variational autoencoder is not directly applicable; it must be extended to sequence modelling. The variational recurrent neural network (VRNN) as proposed in (REF), is a recurrent neural network with a variational autoencoder included at every timestep. A notable addition is that the relationship between latent variables across timesteps is also modelled. 

From this point on $$\mathbf{u}$$ denotes the actuation (in an actuated system). We have a sequence of $$T$$ images $$\mathbf{x}_{1:T}$$, actuations $$\mathbf{u}_{1:T}$$, and the corresponding latent representations of the images $$\mathbf{z}_{1:T}$$. The goal of training this model is to maximise the marginal data likelihood. The general form of the marginal data likelihood is given by:

$$
	p(\mathbf{x}_{1:T} \mid \mathbf{u}_{1:T}) = \int p(\mathbf{x}_{1:T} \mid \mathbf{z}_{1:T} , \mathbf{u}_{1:T}) p(\mathbf{z}_{1:T} \mid \mathbf{u}_{1:T}) \, \text{d}\mathbf{z}_{1:T} \, .
$$

The Newtonian variational autoencoder proposed by (REF) addresses the loss of second-order information by introducing linear second-order dynamics (modelling on the level of acceleration) in the latent space and constraining it such that the dimensions remain disentangled. This aids in interpretability and induces a smooth latent space in which we can successfully use proportional control.

This model explicitly treats position and velocity as two separate latent variables $$\mathbf{z}$$ and $$\mathbf{v}$$. For an actuated rigid body robot of $$D$$ degrees-of-freedom and state vector $$\mathbb{R}^D$$, we model second-order latent dynamics:

$$
	\frac{\text{d}\mathbf{v}}{\text{d}t} = \mathbf{A}(\mathbf{z}, \mathbf{u})  \cdot \mathbf{z} + \mathbf{B}(\mathbf{z}, \mathbf{u}) \cdot  \mathbf{v} + \mathbf{C}(\mathbf{z}, \mathbf{u}) \cdot  \mathbf{u} \, .
$$

Only $$\mathbf{z}$$ is used as the stochastic state variable that is inferred by the approximate posterior $$\mathbf{z}_t \sim q_\phi(\mathbf{z}_t \mid \mathbf{x}_t)$$. Then $$\mathbf{v}$$ is deterministic, naturally defined as:

$$
	\mathbf{v}_t = (\mathbf{z}_t - \mathbf{z}_{t-1}) / \Delta t \, ,
$$


where $$\mathbf{z}_t \sim q_\phi(\mathbf{z}_t \mid \mathbf{x}_t)$$ and $$\mathbf{z}_{t-1} \sim q_\phi(\mathbf{z}_{t-1} \mid \mathbf{x}_{t-1})$$. 

Returning to the marginal data likelihood of the variational recurrent neural network, the generative model is once again factorised under a Markov assumption, with auxiliary latent velocity:

$$
	p(\mathbf{x}_{1:T} \mid \mathbf{z}_{1:T}, \mathbf{u}_{1:T}) = \prod_{t=1}^{T} p(\mathbf{x}_t \mid \mathbf{z}_t)
$$

and,

$$
	p(\mathbf{z}_{1:T} \mid \mathbf{u}_{1:T}; \mathbf{v}_{1:T}) = \prod_{t=1}^{T} p(\mathbf{z}_t \mid \mathbf{z}_{t-1}, \mathbf{u}_{t-1}; \mathbf{v}_t) \, .
$$

The transition prior is chosen to be a Gaussian distributed prediction from the linear dynamical system:

$$
	p(\mathbf{z}_t \mid \mathbf{z}_{t-1}, \mathbf{u}_{t-1}; \mathbf{v}_t) = \mathcal{N}(\mathbf{z}_{t}\mid \mathbf{z}_{t-1} + \Delta t \cdot \mathbf{v}_t, \sigma^2)
$$

$$
	\mathbf{v}_t = \mathbf{v}_{t-1} + \Delta t \cdot (\mathbf{A} \cdot \mathbf{z}_{t-1} + \mathbf{B} \cdot \mathbf{v}_{t-1} + \mathbf{C} \cdot \mathbf{u}_{t-1}) \, ,
$$

with $$[\mathbf{A}, \log(-\mathbf{B}), \log(\mathbf{C})] = \text{diag}(f_\psi(\mathbf{z}_t, \mathbf{u}_t))$$, and $$f_\psi$$ is a neural network. The following additional restrictions are applied:
- **Diagonal transition matrices** $$\{ \mathbf{A, B, C}\}$$ encourages correct coordinate relations because linear combinations of dimensions are eliminated.
- **B is strictly negative** to induce an inertial effect as per the intuition of velocity.
- **C is strictly positive** to ensure correct directional relations between the action $$\mathbf{u}$$ and the position $$\mathbf{z}$$.

The distributions $$p_\theta(\mathbf{x}_t \mid \mathbf{z}_t)$$ and $$q_\phi(\mathbf{z}_t \mid \mathbf{x}_t)$$ are diagonal Gaussians parameterised by neural networks.

This model is also trained by maximising the marginal likelihood of the data. The variational lower bound of this model is given by:

$$
	\begin{split}
		\log p(\mathbf{x}_{1:T} \mid \mathbf{u}_{1:T}) &\geq \sum_{t=1}^T \mathbb{E}_{q(\mathbf{z}_{t-1} \mid \mathbf{x}_{t-1}) q(\mathbf{z}_{t-2} \mid \mathbf{x}_{t-2})} \bigg( \underbrace{\mathbb{E}_{p(\mathbf{\hat{z}}_{t} \mid \mathbf{z}_{t-1}, \mathbf{u}_{t-1})} \log(\mathbf{x}_t \mid \mathbf{\hat{z}}_t)}_{\text{predicted present reconstruction}} \\
		&\hspace{2cm} -\underbrace{\text{KL}(q(\mathbf{z}_t \mid \mathbf{x}_t)||p(\mathbf{z}_t \mid \mathbf{z}_{t-1}, \mathbf{u}_{t-1})}_{\text{transition prior divergence}} \bigg)  \, .
	\end{split}
$$

It is noteworthy that this variational lower bound specifies reconstruction based on the predicted present state $$\mathbf{\hat{z}}_t$$ as per the dynamical model. This is known to place a great emphasis on the predictive capability of the dynamical model through the transition prior. 
