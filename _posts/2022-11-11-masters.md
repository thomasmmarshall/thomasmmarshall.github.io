---
layout: post
title: The limitations of variational models for vision-based robotics
date: 2022-11-11 11:12:00-0400
description: This blog post serves as a severely compressed version of what I did in my Master's thesis.
tags: variational-inference robotics vision-based bayesian
categories: academic
---

# Introduction
We consider the vision-based control of robots. In vision-based robotics, a robot's state is inferred from visual data and utilised in conjunction with a control strategy to carry out intended activities. There are three key elements of vision-based robot control:
- State representation learning
- Dynamical modelling
- Control strategies

These three tasks are often decoupled and determined separately. In addition to learning useful dynamical models, separating these elements requires us to learn action policies through reinforcement learning, or use model predictive control strategies. In [Jaques et al. (2021)](https://arxiv.org/abs/2006.01959), the authors argue that decoupling these elements places unnecessary computational burden on control. Instead, they propose considering them jointly, by introducing additional inductive priors that simplify downstream control. The resultant model allows us to apply well-known feedback control using the embeddings.

In the rest of this blog post, we'll briefly go over the three aforementioned things that we need to control robots from vision. We'll start with a very quick recap of basic control theory.


# Feedback control
In real robotic control scenarios where we have access to proprioceptive information, it is typical to use simple closed-loop control to direct our process variables $$y(t)$$ towards their setpoints $$r(t)$$ using the error $$e(t)$$. The process variable (also called the state) can be a scalar or vector quantity. When the state is a vector of $$\mathbb{R}^D$$, the error will also be a vector of $$\mathbb{R}^D$$; depending on the scenario this can be considered as $$D$$ parallel control loops (there might be some correlation between state variables depending on the situation)

[<img src="/assets/img/feedback.jpg" width="750"/>](/assets/img/feedback.jpg)


The typical controller used in feedback control is the proportional-integral-derivative (PID) controller:

$$
u(t) = K_\text{p} \, e(t) + K_\text{i} \, \int_0^t e(\tau) \, \text{d}\tau + K_\text{d} \, \frac{\text{d} \, e(t)}{\text{d}t} \, .
$$

The gain terms $$K_\text{p}, K_\text{i} \text{ and } K_\text{d}$$ contribute distinctively to the response of the controller: the proportional term guides the process variable directly towards its setpoint; the derivative term introduces inertia in the actuation, leading to smoother control trajectories; and the integral term is generally used to eliminate steady-state error.

The use of PID control is widespread and considered to be the golden standard for simple feedback control because it incorporates the proportional, derivative and integral modes and it is simple to implement and tune.


# Variatonal state inference
Now, we look to develop a model that allows us to learn low-dimensional representations of images of robots that we can directly use for control. The core of variational methods for latent state representation learning is the variational autoencoder (VAE), which allows us to perform efficient approximate inference and learning in the presence of continuous latent variables with intractable posteriors.

## Variational autoencoder
The variational autoencoder (REF) is a popular framework for performing efficient approximate inference and learning in directed probabilistic models with continuous latent variables and intractable posterior distributions. It is customary that the embedding dimensionality is less than the data dimensionality, so the learned representation is a compression.

The posterior inference model $$q_\phi (\mathbf{z \mid x})$$ is called the probabilistic \textit{encoder}, and $$p_\theta (\mathbf{x \mid z})$$ the probabilistic \textit{decoder}; we will continue to use these terms in this document. The parameters $$\phi$$ of the variational approximation $$q_\phi (\mathbf{z \mid x})$$ to the intractable true posterior is learned jointly with the generative model parameters $$\theta$$. This amortised variational Bayesian approach allows us to infer $$\mathbf{z}$$ from $$\mathbf{x}$$ cheaply, using the surrogate posterior distribution $$q_\phi (\mathbf{z \mid x})$$.

The objective in this generative model is to maximise the aggregate marginal likelihood over the data set $\sum_{i=1}^N \log p_\theta (\mathbf{x}_i)$. To maximise this, we must first find the marginal:
$$
	\log p_\theta (\mathbf{x}) = \log \int p_\theta(\mathbf{x} \mid \mathbf{z}) p_\theta(\mathbf{z}) \, \text{d} \mathbf{z} \, .
$$

We multiply with the approximate posterior distribution $q_\phi (\mathbf{z \mid x})$ inside the integral without changing its value, and apply the definition of expectations:

$$
	\begin{split}
		\log p_\theta (\mathbf{x}) &= \log \int p_\theta(\mathbf{x} \mid \mathbf{z}) p_\theta(\mathbf{z}) \frac{q_\phi (\mathbf{z \mid x})}{q_\phi (\mathbf{z \mid x})} \, \text{d} \mathbf{z}\\
		&= \log \left( \mathbb{E}_{q_\phi (\mathbf{z \mid x})} p_\theta (\mathbf{z}) \frac{p_\theta (\mathbf{x \mid z})}{q_\phi (\mathbf{z \mid x})} \right) \, .
	\end{split}
$$

We apply Jensen's inequality, the log identities and the definition of Kullback-Leiber (KL) divergence:

$$
	\begin{split}
		\log p_\theta (\mathbf{x}) &\geq \mathbb{E}_{q_\phi (\mathbf{z \mid x})} \left[ \log p_\theta (\mathbf{x \mid z})  + \log p_\theta (\mathbf{z}) - \log q_\phi (\mathbf{z \mid x}) \right] \\
		\mathcal{L}(\theta,\phi;\mathbf{x}) &= \mathbb{E}_{q_\phi (\mathbf{z \mid x})} \left[ \log p_\theta (\mathbf{x \mid z})\right] -\text{KL}(q_\phi(\mathbf{z \mid x}) || p_\theta (\mathbf{z})) \, .
	\end{split}
$$

## Variational recurrent neural network

## Newtonian variational autoencoder

